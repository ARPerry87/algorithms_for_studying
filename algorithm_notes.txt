Euclids algorithm (GCD)
Essentially like untangling headphones, keep dividing until you find the greatest common demonenator 

input: two integers m and n, >= 0 and not both 0
while n! = 0 {
    r = m mod n
    m = n 
    n = r
}
return m

in ruby 

while n!= 0
  r = m % n
  m = n
  n = 4
end
return m 

steps for algorithm design: 
1. Understand the problem
2. Figure out what your computer is capable of (sequential vs parallel)
3. Choose exact or approximate
4. Pick a data structure 
5. Consider algorithm design techniques (Prove correctness, analyze algorithm, code it)

Important problem types: 
Sorting
Searching
String processing
Graph Problems
Combinatorial
Geometric 
Numeric

Time effenciency is how fast the algorithm takes to run
Space effenciency is how much space the algorithm needs to compute the result

Basic operation: 
The most commonly used operation 

Order of Growth
c, log n, log(n^2), (log n), n^(1/3), n^(1/2), n, n(log n) ,n^2, n^3, 2^n, n!

For binary search these are the steps using First and Last index in a sorted array: 
arr = [2, 4, 5, 5, 5, 5, 6, 8, 10]

For binary search we want to have two pointers, one left, and one right. 
The left  pointer should start at the index[0] aka the start of the array, and should go until the mid.
The right pointer should go from the mid to the end of the array, so say we have a target of 5.

this would then look like this: 

def binary_search(arr, target)
    return [-1,-1] if arr.empty? # or zero if the constraint calls for it
    
    l, r = 0, arr.length - 1

    while l <= r
        mid = (l + r) / 2

        if arr[mid] == target
            return mid
        elsif arr[mid] > target
            r = mid - 1
        else
            l = mid + 1
        end
    end

    return -1
end

# Example usage:
arr = [2, 4, 5, 5, 5, 5, 6, 8, 10]
target = 5
puts binary_search(arr, target)  # Output: Index of one of the 5s, e.g., 2

For problems like the kth largets element, where k = n and an arr is being looped over, you would want to find the largest element after the k amount of removals takes place.

example: 
[4, 2, 9, 7, 5, 6, 7, 1, 3]
k = 4

we want to get the k - 1 so we only get rid of 3 so that our 4th largest is returned

We find the largets element first [9] and remove that 
Then the second [7] and remove that 
Third is [7]
4th largest and our target = [6]

we should use ruby's heapify gem in order to easily heapfiy the array as we want the maximum values to be popped off the top of the heap until k - 1

we should use a priority queue, as a sort is fast but will lag in certain situations

require 'heapify'

def kth_largest(arr, k)
  # Transform the array to negative values to use min-heap as max-heap
  arr.map! { |elem| -elem }
  
  # Heapify the array
  arr.heapify!
  
  # Pop k-1 elements from the heap
  (k - 1).times { arr.heap_pop }
  
  # Return the k-th largest element (convert back to positive)
  return -arr.heap_pop
end

For a symmetric tree question, aka we want to see if it's a mirror of itself 

def are_symmetric(root1, root2)
  if root1.nil? && root2.nil? # if both subtrees are empty, they are symmetric
    return true
  elsif (root1.nil? != root2.nil?) || (root1.val != root2.val) # if one subtree is empty and the other is not, or if the values are not the same, they are not symmetric
    return false
  else
    # recursively check if the left subtree of root1 is symmetric with the right subtree of root2 and vice versa
    return are_symmetric(root1.left, root2.right) && are_symmetric(root1.right, root2.left)
  end  
end

def is_symmetric(root)
  return true if root.nil? # an empty tree is symmetric
  return are_symmetric(root.left, root.right) # check if the left and right subtrees are symmetric
end

Using backtracking in a problem
The generate paranthesis problem 

Given an integer n, generate all the valid combinations of n pairs of parentheses

def generate(n)
  def rec(n, diff, comb, combs) # we use a nested method to make it easier to call
    if diff < 0 # if diff is less than 0 then we want to return to stop the recursion
      return
    elsif n == 0 # else if n is 0 and diff is 0 then add the current combo to the list of combos
      if diff == 0
        combs << comb.join
      end
    else # we add an opening parentheses '(', increment diff by 1, and do that because we have increased the diff between the opening and closing, then recursively call rec
      comb << '('
      rec(n - 1, diff + 1, comb, combs)
      comb.pop # remove the paran, we do this so we can backtrack and try adding a closing parentheses
      comb << ')' # then we add a closing paranthesis ')' and decrement diff by 1 and recrusively call rec
      rec(n - 1, diff - 1, comb, combs) # we decrement diff by one because a closing parentheses decreases the difference between the number of opening and closing parens
      comb.pop # then we remove the last added paran
    end
  end

  combs = []
  rec(2 * n, 0, [], combs)
  combs
end

backtracking algorithms follow a general outline of how they work: 
1. choose an initial solution
2. Explore all possible extensions of the current situation
3. If an extension leads to a solution, return that solution. 
4. If an extension does not lead to a solution, backtrack to the previous position and try a different extension. 
5. Repeat steps 2.4 until all possible solutions have been reached. 

Another example: Finding the shortest path through a maze 

Input: a maze represented as a 2d array where 0 represents an open space and 1 a wall. 

1. Start at the starting point 
2. For each of the four possible directions try moving in that direction
3. if moving in that direction leads to the end point, return the path taken
4. If moving in that direction does not lead to the end, backtrack to the previous position and try a different direction
5. Repeat steps 2-4 until the ending point is reached or all paths have been exhausted. 

def shortest_path_through_maze(maze, start, end)
  def rec(x, y, maze, path, shortest_path)
    # Base case: if we reach the end point
    if [x, y] == end
      if shortest_path.empty? || path.length < shortest_path.length
        shortest_path.replace(path.dup)
      end
      return
    end

    # Directions: right, down, left, up
    directions = [[0, 1], [1, 0], [0, -1], [-1, 0]]

    directions.each do |dx, dy|
      nx, ny = x + dx, y + dy

      # Check if the new position is within bounds and is an open space
      if nx >= 0 && ny >= 0 && nx < maze.length && ny < maze[0].length && maze[nx][ny] == 0
        # Mark the position as visited
        maze[nx][ny] = 2
        path << [nx, ny]

        # Recur with the new position
        rec(nx, ny, maze, path, shortest_path)

        # Backtrack: unmark the position and remove from path
        maze[nx][ny] = 0
        path.pop
      end
    end
  end

  shortest_path = []
  start_x, start_y = start
  end_x, end_y = end

  # Mark the starting position as visited
  maze[start_x][start_y] = 2
  rec(start_x, start_y, maze, [[start_x, start_y]], shortest_path)

  shortest_path
end

DFS
What is dfs?
DFS or depth-first-search is a recursive algorithm that uses the backtracking principle.
It entails conducting exhaustive searches of all nodes by moving forward if possible and backtracking if necessary.
To visit the next node, pop the top node from the stack and push all of its nearby nodes into the stack.
Topological sorting, scheduling problems, graph cycle detection, and solving puzzles with just one solution such as a maze or sudoku all employ dfs.
Other applications include network analysis, such as determining if a graph is bipartite. 

Graph traversal is a technicque for finding a vertex in a graph, in the search, graph traversal is also used to determine the order in which it visits its vertices.
The two techniques are dfs and bfs respectively. 

DFS explores or traverses datastructures, such as trees and graphs. 

You start at the root node, in case of a graph you can use any node as a root, and examines each branch as far down as possible before backtracking.
When a deadend occurs in any iteration, the DFS method traverses a network in a deathward motion, and uses a stack to remember to acquire the next vertex. 

Outcomes of DFS are spanning trees, which is a graph that is devoid of loops. To implement DFS you need the stack structure with a maximum size equal to the total number of vertices in the graph. 
To implement you need the following steps: 

1. Create a stack with the total number of vertices in the graph as the size. 
2. Choose any vertex as the traversal's beginning point. Push a visit to that vertex and add it to the stack. 
3. Push any non-visited adjacent vertices of a vertex as the top of the stack to the top of the stack. 
4. Repeat steps 3 and 4 until there's no more vertices to visit from the vertex at the top of hte stack. 
5. If there are no new vertices to visit, go back and pop one from the stack using backtracking. 
6. Continue using steps 3, 4, and 5 until the stack is empty. 
7. When the stack is entirely unoccupied create a final spanning tree by deleting the graphs unused edges. 

We should keep a visited array so we don't visit a vertex again or create an infinite loop. 

BFS 
Breadth First Search is a fundemental graph traversal algorithm. It begins with a node then first traverses all adjacent nodes. 
Once all adjacent are visited, then their adjacent are traversed. This is different from DFS in a way that closest vertices are visited before others. 
We mainly traverse vertices level by level. A lot of popular graph algorithms like Dijkstra's, Kahn's, and Prim's are based on BFS. 
BFS can be used to detect cycle in a directed and undirected graph, find shortest path in an unweighted graph, and many more problems. 

BFS from a given source: 
The algo starts from a given source and explores all reachable vertics from the given source. It is similar to BF-traversal of a tree. 
Like a tree we begin with the given source and traverse vertices level by level using a queue. The only catch is that, unlike trees, graphs may contain cycles, so we may come to the same node again.
To avoid processing a node more than once we use a boolean visited array. 

initialization: Enqueu the given source vertex into a queue and mark it as visited. 
1. Exploration, while the queue is not empty dequeue a node from the queue and visit it. For each unvisited neighbor of the dequeued node we'll enqueue the neighbor and mark the neighbor as visited.
2. Termination: repeat step 2 until the queue is empty. 

This ensures that all nodes in the graph are visited in a bfs manner, starting from the starting node. 

Example of BFS: 


DIRECTIONS = [[0, 1], [1, 0], [0, -1], [-1, 0]]

def breadth_first_search(map, start, target)
  # Include an edge case
  return 0 if start == target

  width, height = map[0].size, map.size

  queue = [start]
  next_queue = []
  visited = []

  steps = 0

  # We're going to walk through the queue until it's empty
  while !queue.empty?
    # Loop over each item in queue
    queue.each do |position|
      x, y = position

      # Did we visit this already? If yes, skip
      next if visited.include?(position)
      visited.push(position)

      # Is this a wall in the maze? If so, skip
      next if map[x][y] == '#'

      # Have we reached the target? If yes, return steps
      return steps if target == position

      # Try to take another step in each direction
      DIRECTIONS.each do |dx, dy|
        new_x, new_y = x + dx, y + dy

        # Check if we're out of bounds
        next if new_x < 0 || new_y < 0
        next if new_x >= width || new_y >= height

        # Add the new position to the next queue
        next_queue << [new_x, new_y]
      end
    end

    # Now we'll swap the queues, and start processing the next_queue
    # and replace next_queue with a new empty queue
    queue, next_queue = next_queue, []
    
    # Since this is BFS, we're going to increment the steps
    # as we move through the steps in order.
    steps += 1
  end
  # Unsolvable map if we get here
  return -1
end

To run BFS on a graph that may be disconnected, you need to ensure you visit all nodes, even those that are not reachable from the starting node. 
This can be achieved by iterating over all nodes and performing bfs from each unvisited node. 

Initialize: a set to keep track of visited nodes, a queue for the bfs traversal. 
Iterate over all nodes: if a node has not been visited, perform a bfs from that node. 
BFS function: use a queue to perform the bfs iteratively, and mark nodes as visited as they're processed. 

Example in ruby: 

require 'set'

def bfs(graph)
  visited = Set.new

  graph.keys.each do |node|
    if !visited.include?(node)
      bfs_visit(graph, node, visited)
    end  
  end
end

def bfs_visit(graph, node, visited)
  queue = [node]
  while !queue.empty?
    node = queue.shift

    next if visited.include?(node)
    visited.add(node)

    puts node

    queue[node].each do |neighbor|
      queue.push(neighbor) unless visited.include?(neighbor)
    end
  end    
end

Merge Sort: 
A sorting algorithm that uses the divide and conquer strategy to effencitly sort a list of elements in ascending order.
Steps: 
1. Divide: Split the list into two halves 
2. Conquer: repeat the vidivde step on each half until each sublist has only one element. 
3. Combine: Merge the sublists back together, putting the lowest value first to create a sorted list.

The merge sort algo is recursive, meaning that the function calls itself. The time complexity is O(nlog(n))
which means it scales well as the numbers of elements increase. 

You can think of merge sort like a organizing a deck of cards. You divide the deck in half, then in half again, until you have a single card, then you merge the cards back together two at a time, then four at a time, and so on.

Example: 
[38, 27, 43, 10]
[38, 27], [43, 10]
[38] [27] [43] [10]

Conquer: 
They are all already sorted in their individual arrays

Merge: 
Merge 38 and 27, to get [27, 38], and 43 and 10 to get [10, 43]
Merge the two arrays to get [10, 27, 38, 43]

Code Example: 
def merge_sort(arr)
  return arr if arr.length <= 1 # if there's only 1 element return the array
  mid = (arr.length / 2).floor # set our mid/partition to the length divided by 2 and set floor
  left = merge_sort(arr[0...mid]) # our left array equals our merge_sort function, fed arr, and going from 0...mid
  right = merge_sort(arr[mid...arr.length])

  merge(left, right)
end

def merge(left, right)
  sorted = []
  until left.empty? || right.empty?
    if (left.first <=> right.first) == -1
      sorted << left.shift
    else
      sorted << right.shift
    end
  end
  sorted + left + right
end

arr = [1, 45, 3, 22, 56, 89, 100, 12, 15]

If you are doing a problem that asks finding the nodes at distance k or with equal number of hops or steps it is always bfs

Quicksort: 
Quick sort is a highly efficient sorting algo that uses the divide and conquer strategy to arrange elements 
in order by selecting a pivot element, partitioning the array into elements smaller and larger than the pivot, 
and then recrusively sorting each partition until the array is sorted.
Essentially it repeatedly splits the data into smaller sub-arrays based on a chosen pivot value.
It is faster for larger databases. 

Three steps to quicksort
1. choose a pivot, select an element from the array as the pivot. the choice of pivot can vary.
2. Partition the array, rearrange the array around the pivot. After partitioning, all elements smaller than the pivot 
will be on its left, and all elements greater than the pivot will be on its right. The pivot is then in its correct position and we can get the index.
3. Recursively call, recursively apply the same process to the two partitioned sub-arrays (left and right of pivot)
4. Base case: The recursion stops when there is only one element left in the sub-array, as a single element is already sorted

Example of Quicksort in ruby:

def quicksort(arr)
  return [] if arr.empty?

  temp = arr.dup

  pivot = temp.delete_at(rand(temp.length))
  left, right = temp.partition{|i| i < pivot }

  return [quicksort(left), pivot, quicksort(right)].flatten
end

